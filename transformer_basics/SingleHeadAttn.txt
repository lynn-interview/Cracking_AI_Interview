"""a fundamental block of constructing self-attention.
BUT it is not what a real self-attention block may look like in practice.
The only reason for having it here is for interview [cauz you may be asked to do it]
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class SingleHeadAttn(nn.Module):
	def __init__(self, embed_dim, num_heads):
		super().__init__()
		self.embed_dim = embed_dim
		
		""" 
		embedding dimension (aka. the 175B model parameter size = num_heads * each_head_dim
		method A: self.head_dim =  int(embed_dim / num_heads) # habits from a previous C++ coder
		method A_v2: self.head_dim =  embed_dim // num_heads # more pythonic
		method A_DONT: self.head_dim =  round(embed_dim / num_heads) 
		#cauz it may round up and makes  num_heads * self.head_dim >embed_dim

		an elegent and sustainable habit is to always add an assertation
		so here comes the final lines below
		"""
		assert embed_dim % num_heads == 0, f"embed_dim ({embed_dim}) MUST be divided by num_heads ({num_heads})"
		self.head_dim =  embed_dim // num_heads

		self.q_proj = nn.Linear(self.embed_dim, self.head_dim, bias=False) #y=xW^T
		self.k_proj = nn.Linear(self.embed_dim, self.head_dim, bias=False) 
		self.v_proj = nn.Linear(self.embed_dim, self.head_dim, bias=False) 

	def forward(self, x, mask=None):
		q = self.q_proj(x)
		k = self.k_proj(x)
		v = self.v_proj(x)

		attn_scr = q@k.transpose(-2, -1)
		scale = 1/math.sqrt(self.head_dim)
		attn_scr = attn_scr * scale
		if mask is not None:
			attn_scr = attn_scr.masked(mask ==0, float('-inf'))
		attn_sft = F.softmax(attn_scr, dim=-1)
		
		return attn_sft@ v